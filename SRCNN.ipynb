{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNoY5WNCz3zxj7sw/w/eigf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/westjiuuu/SRCNN/blob/main/SRCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGyf9pJKGMCn",
        "outputId": "d37a61b5-f594-40a2-abfe-0df84a07bafe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SRCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SRCNN, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=9, padding=0),  # ‚úÖ padding Ï†úÍ±∞\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 32, kernel_size=1, padding=0),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 1, kernel_size=5, padding=0)   # ‚úÖ padding Ï†úÍ±∞\n",
        "        )\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.normal_(m.weight, mean=0.0, std=0.001)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)"
      ],
      "metadata": {
        "id": "5GksjutgGSUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [2] H5 Dataset Î°úÎî© (Ï†ïÍ∑úÌôî Ìè¨Ìï®)\n",
        "\n",
        "import h5py\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class H5Dataset(Dataset):\n",
        "    def __init__(self, file_path):\n",
        "        with h5py.File(file_path, 'r') as f:\n",
        "            self.inputs = f['lr'][:] / 255.0  # (N, 33, 33)\n",
        "            self.labels = f['hr'][:] / 255.0  # (N, 21, 21)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.from_numpy(self.inputs[idx]).unsqueeze(0).float()\n",
        "        y = torch.from_numpy(self.labels[idx]).unsqueeze(0).float()\n",
        "        return x, y\n"
      ],
      "metadata": {
        "id": "eaKoU39KGUeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# ÏÑ§Ï†ï\n",
        "train_file = '/content/drive/MyDrive/SRCNN/91-image_x3_blur.h5'\n",
        "batch_size = 16\n",
        "num_epochs = 400\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎî©\n",
        "train_dataset = H5Dataset(train_file)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Î™®Îç∏ Ï†ïÏùò\n",
        "model = SRCNN().to(device)\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': model.model[0].parameters(), 'lr': 1e-4},\n",
        "    {'params': model.model[2].parameters(), 'lr': 1e-4},\n",
        "    {'params': model.model[4].parameters(), 'lr': 1e-5},\n",
        "])\n",
        "\n",
        "# ÌïôÏäµ Î£®ÌîÑ (epoch Í∏∞Ï§Ä)\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\"):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    loss_history.append(avg_loss)\n",
        "    print(f\"[Epoch {epoch}/{num_epochs}] Loss: {avg_loss:.6f}\")\n",
        "\n",
        "# ‚úÖ Î™®Îç∏ Ï†ÄÏû•\n",
        "torch.save(model.state_dict(), 'srcnn_x3_epoch400.pth')\n",
        "\n",
        "# ‚úÖ Loss Í∑∏ÎûòÌîÑ ÏãúÍ∞ÅÌôî\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, num_epochs + 1), loss_history, marker='o', linewidth=1.5)\n",
        "plt.title(\"Training Loss over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE Loss\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qM6IBXF3GWE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from skimage.metrics import peak_signal_noise_ratio as compare_psnr\n",
        "from skimage.metrics import structural_similarity as compare_ssim\n",
        "\n",
        "# [1] ÌèâÍ∞ÄÏö© Dataset ÌÅ¥ÎûòÏä§\n",
        "class H5EvalDataset(Dataset):\n",
        "    def __init__(self, file_path):\n",
        "        self.input_images = []\n",
        "        self.label_images = []\n",
        "\n",
        "        with h5py.File(file_path, 'r') as f:\n",
        "            lr_group = f['lr']\n",
        "            hr_group = f['hr']\n",
        "\n",
        "            for key in lr_group.keys():\n",
        "                lr_img = lr_group[key][()] / 255.0\n",
        "                hr_img = hr_group[key][()] / 255.0\n",
        "\n",
        "                self.input_images.append(torch.tensor(lr_img).unsqueeze(0).float())\n",
        "                self.label_images.append(torch.tensor(hr_img).unsqueeze(0).float())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_images[idx], self.label_images[idx]\n",
        "\n",
        "\n",
        "# ‚úÖ Ï§ëÏã¨ crop Ìï®Ïàò\n",
        "def center_crop(img, target_h, target_w):\n",
        "    h, w = img.shape\n",
        "    top = (h - target_h) // 2\n",
        "    left = (w - target_w) // 2\n",
        "    return img[top:top + target_h, left:left + target_w]\n",
        "\n",
        "\n",
        "# [2] ÌèâÍ∞Ä Ìï®Ïàò (PSNR, SSIM)\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    total_psnr_bic, total_psnr_srcnn = 0.0, 0.0\n",
        "    total_ssim_bic, total_ssim_srcnn = 0.0, 0.0\n",
        "    n = len(dataloader.dataset)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            output = model(x).clamp(0.0, 1.0)\n",
        "\n",
        "            # numpy Î≥ÄÌôò\n",
        "            input_np  = x.squeeze().cpu().numpy() * 255.0\n",
        "            output_np = output.squeeze().cpu().numpy() * 255.0\n",
        "            label_np  = y.squeeze().cpu().numpy() * 255.0\n",
        "\n",
        "            # Ï∂úÎ†• ÌÅ¨Í∏∞Ïóê ÎßûÏ∂∞ crop\n",
        "            h_out, w_out = output_np.shape\n",
        "            input_np  = center_crop(input_np,  h_out, w_out)\n",
        "            label_np  = center_crop(label_np,  h_out, w_out)\n",
        "\n",
        "            # PSNR & SSIM\n",
        "            psnr_bic  = compare_psnr(label_np, input_np, data_range=255)\n",
        "            psnr_src  = compare_psnr(label_np, output_np, data_range=255)\n",
        "            ssim_bic  = compare_ssim(label_np, input_np, data_range=255)\n",
        "            ssim_src  = compare_ssim(label_np, output_np, data_range=255)\n",
        "\n",
        "            total_psnr_bic   += psnr_bic\n",
        "            total_psnr_srcnn += psnr_src\n",
        "            total_ssim_bic   += ssim_bic\n",
        "            total_ssim_srcnn += ssim_src\n",
        "\n",
        "    print(\"üìä Set5 ÌèâÍ∞Ä Í≤∞Í≥º:\")\n",
        "    print(f\"üìå Avg PSNR - Bicubic: {total_psnr_bic / n:.2f} dB / SRCNN: {total_psnr_srcnn / n:.2f} dB\")\n",
        "    print(f\"üìå Avg SSIM - Bicubic: {total_ssim_bic / n:.4f} / SRCNN: {total_ssim_srcnn / n:.4f}\")\n",
        "\n",
        "\n",
        "# Î™®Îç∏ Î°úÎìú\n",
        "model.load_state_dict(torch.load('/content/srcnn_x3_epoch400.pth', map_location=device))\n",
        "model.to(device)\n",
        "\n",
        "# ÌèâÍ∞Ä Îç∞Ïù¥ÌÑ∞ Î°úÎçî\n",
        "eval_file = '/content/drive/MyDrive/SRCNN/Set5_x3_blur.h5'\n",
        "eval_dataset = H5EvalDataset(eval_file)\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# ÌèâÍ∞Ä Ïã§Ìñâ\n",
        "evaluate_model(model, eval_loader)"
      ],
      "metadata": {
        "id": "y7FsM7xoGcpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from skimage.metrics import peak_signal_noise_ratio as compare_psnr\n",
        "from skimage.metrics import structural_similarity as compare_ssim\n",
        "\n",
        "# [1] ÌèâÍ∞ÄÏö© Dataset ÌÅ¥ÎûòÏä§\n",
        "class H5EvalDataset(Dataset):\n",
        "    def __init__(self, file_path):\n",
        "        self.input_images = []\n",
        "        self.label_images = []\n",
        "\n",
        "        with h5py.File(file_path, 'r') as f:\n",
        "            lr_group = f['lr']\n",
        "            hr_group = f['hr']\n",
        "\n",
        "            for key in lr_group.keys():\n",
        "                lr_img = lr_group[key][()] / 255.0  # ‚úÖ Ï†ïÍ∑úÌôî\n",
        "                hr_img = hr_group[key][()] / 255.0  # ‚úÖ Ï†ïÍ∑úÌôî\n",
        "\n",
        "                self.input_images.append(torch.tensor(lr_img).unsqueeze(0).float())\n",
        "                self.label_images.append(torch.tensor(hr_img).unsqueeze(0).float())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_images[idx], self.label_images[idx]\n",
        "\n",
        "\n",
        "# 6ÌîΩÏÖÄ crop Ìï®Ïàò\n",
        "def shave(img, border=6):\n",
        "    return img[border:-border, border:-border]\n",
        "\n",
        "\n",
        "# [2] ÌèâÍ∞Ä Ìï®Ïàò (PSNR, SSIM)\n",
        "def evaluate_model(model, dataloader, shave_border=6):\n",
        "    model.eval()\n",
        "    total_psnr_bic, total_psnr_srcnn = 0.0, 0.0\n",
        "    total_ssim_bic, total_ssim_srcnn = 0.0, 0.0\n",
        "    n = len(dataloader.dataset)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            # Bicubic = ÏûÖÎ†• Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©\n",
        "            bicubic = x\n",
        "\n",
        "            # SRCNN Ï∂úÎ†•\n",
        "            output = model(x).clamp(0.0, 1.0)\n",
        "\n",
        "            # numpy Î≥ÄÌôò + 6ÌîΩÏÖÄ crop\n",
        "            bicubic_np = shave(bicubic.squeeze().cpu().numpy() * 255.0, border=shave_border)\n",
        "            output_np  = shave(output.squeeze().cpu().numpy() * 255.0, border=shave_border)\n",
        "            label_np   = shave(y.squeeze().cpu().numpy() * 255.0, border=shave_border)\n",
        "\n",
        "            # PSNR & SSIM Í≥ÑÏÇ∞\n",
        "            psnr_bic  = compare_psnr(label_np, bicubic_np, data_range=255)\n",
        "            psnr_src  = compare_psnr(label_np, output_np,  data_range=255)\n",
        "            ssim_bic  = compare_ssim(label_np, bicubic_np, data_range=255)\n",
        "            ssim_src  = compare_ssim(label_np, output_np,  data_range=255)\n",
        "\n",
        "            total_psnr_bic   += psnr_bic\n",
        "            total_psnr_srcnn += psnr_src\n",
        "            total_ssim_bic   += ssim_bic\n",
        "            total_ssim_srcnn += ssim_src\n",
        "\n",
        "    print(\"üìä Set5 H5 ÌèâÍ∞Ä Í≤∞Í≥º (crop Ï†ÅÏö©):\")\n",
        "    print(f\"üìå Avg PSNR - Bicubic: {total_psnr_bic / n:.2f} dB / SRCNN: {total_psnr_srcnn / n:.2f} dB\")\n",
        "    print(f\"üìå Avg SSIM - Bicubic: {total_ssim_bic / n:.4f} / SRCNN: {total_ssim_srcnn / n:.4f}\")\n",
        "\n",
        "\n",
        "# --- Î™®Îç∏ Î°úÎìú ---\n",
        "model.load_state_dict(torch.load('/content/srcnn_x2_epoch200.pth', map_location=device))\n",
        "model.to(device)\n",
        "\n",
        "# --- ÌèâÍ∞Ä Îç∞Ïù¥ÌÑ∞ Î°úÎçî ---\n",
        "eval_file = '/content/drive/MyDrive/SRCNN/Set5/Set5_x2.h5'\n",
        "eval_dataset = H5EvalDataset(eval_file)\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# --- ÌèâÍ∞Ä Ïã§Ìñâ ---\n",
        "evaluate_model(model, eval_loader, shave_border=6)\n"
      ],
      "metadata": {
        "id": "TuqHhKpbybDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import h5py\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ÏÑ§Ï†ï\n",
        "scale = 3\n",
        "patch_size = 33         # ÏûÖÎ†• (LR) Ìå®Ïπò ÌÅ¨Í∏∞\n",
        "label_size = 21         # Ï∂úÎ†• (SRCNN) ÌÅ¨Í∏∞ ‚Üí HR crop ÌÅ¨Í∏∞\n",
        "stride = 14\n",
        "\n",
        "t91_dir = '/content/drive/MyDrive/SRCNN/T91_img'\n",
        "set5_dir = '/content/drive/MyDrive/SRCNN/Set5_img'\n",
        "\n",
        "t91_output = '/content/drive/MyDrive/SRCNN/91-image_x3_nopad.h5'\n",
        "set5_output = '/content/drive/MyDrive/SRCNN/Set5_x3_nopad.h5'\n",
        "\n",
        "\n",
        "def convert_rgb_to_y(img):  # Y Ï±ÑÎÑê Ï∂îÏ∂ú\n",
        "    img = img.astype(np.float32)\n",
        "    return 16.0 + (65.481 * img[..., 0] + 128.553 * img[..., 1] + 24.966 * img[..., 2]) / 255.0\n",
        "\n",
        "\n",
        "# ‚úÖ ÌïôÏäµÏö© Ï†ÑÏ≤òÎ¶¨ (91 Ïù¥ÎØ∏ÏßÄ ‚Üí patch Í∏∞Î∞ò, padding ÏóÜÏùå Íµ¨Ï°∞Ïóê ÎßûÏ∂§)\n",
        "def preprocess_t91():\n",
        "    lr_patches = []\n",
        "    hr_patches = []\n",
        "\n",
        "    print(\"üì¶ 91Í∞ú Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ Ï§ë (padding ÏóÜÏùå)...\")\n",
        "    for img_path in tqdm(sorted(glob.glob(os.path.join(t91_dir, '*')))):\n",
        "        hr = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # mod crop\n",
        "        hr_width = (hr.width // scale) * scale\n",
        "        hr_height = (hr.height // scale) * scale\n",
        "        hr = hr.resize((hr_width, hr_height), resample=Image.BICUBIC)\n",
        "\n",
        "        # LR ÏÉùÏÑ±\n",
        "        lr = hr.resize((hr_width // scale, hr_height // scale), resample=Image.BICUBIC)\n",
        "        lr = lr.resize((hr_width, hr_height), resample=Image.BICUBIC)\n",
        "\n",
        "        # Y Ï±ÑÎÑê Ï∂îÏ∂ú\n",
        "        hr = convert_rgb_to_y(np.array(hr))\n",
        "        lr = convert_rgb_to_y(np.array(lr))\n",
        "\n",
        "        # patch Ï∂îÏ∂ú\n",
        "        offset = (patch_size - label_size) // 2  # Ïòà: 6\n",
        "        for i in range(0, lr.shape[0] - patch_size + 1, stride):\n",
        "            for j in range(0, lr.shape[1] - patch_size + 1, stride):\n",
        "                lr_patch = lr[i:i+patch_size, j:j+patch_size]\n",
        "                hr_patch = hr[i+offset:i+offset+label_size, j+offset:j+offset+label_size]  # Ï§ëÏïô crop\n",
        "\n",
        "                lr_patches.append(lr_patch)\n",
        "                hr_patches.append(hr_patch)\n",
        "\n",
        "    # Ï†ÄÏû•\n",
        "    with h5py.File(t91_output, 'w') as f:\n",
        "        f.create_dataset('lr', data=np.array(lr_patches))\n",
        "        f.create_dataset('hr', data=np.array(hr_patches))\n",
        "    print(f\"‚úÖ Ï†ÄÏû• ÏôÑÎ£å: {t91_output}\")\n",
        "\n",
        "\n",
        "# ‚úÖ ÌèâÍ∞ÄÏö© Ï†ÑÏ≤òÎ¶¨ (Set5 Ï†ÑÏ≤¥ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû• - Í∑∏ÎåÄÎ°ú)\n",
        "def preprocess_set5():\n",
        "    print(\"üì¶ Set5 Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ Ï§ë (Ï†ÑÏ≤¥ Ï†ÄÏû•)...\")\n",
        "    with h5py.File(set5_output, 'w') as f:\n",
        "        f.create_group('lr')\n",
        "        f.create_group('hr')\n",
        "\n",
        "        for i, img_path in enumerate(sorted(glob.glob(os.path.join(set5_dir, '*')))):\n",
        "            hr = Image.open(img_path).convert('RGB')\n",
        "\n",
        "            # mod crop\n",
        "            hr_width = (hr.width // scale) * scale\n",
        "            hr_height = (hr.height // scale) * scale\n",
        "            hr = hr.resize((hr_width, hr_height), resample=Image.BICUBIC)\n",
        "\n",
        "            # LR ÏÉùÏÑ±\n",
        "            lr = hr.resize((hr_width // scale, hr_height // scale), resample=Image.BICUBIC)\n",
        "            lr = lr.resize((hr_width, hr_height), resample=Image.BICUBIC)\n",
        "\n",
        "            # Y Ï±ÑÎÑê Ï∂îÏ∂ú\n",
        "            hr = convert_rgb_to_y(np.array(hr))\n",
        "            lr = convert_rgb_to_y(np.array(lr))\n",
        "\n",
        "            f['hr'].create_dataset(str(i), data=hr)\n",
        "            f['lr'].create_dataset(str(i), data=lr)\n",
        "\n",
        "    print(f\"‚úÖ Ï†ÄÏû• ÏôÑÎ£å: {set5_output}\")\n",
        "\n",
        "\n",
        "# Ïã§Ìñâ\n",
        "preprocess_t91()\n",
        "preprocess_set5()\n"
      ],
      "metadata": {
        "id": "5TsOncjgzkKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import h5py\n",
        "import numpy as np\n",
        "from PIL import Image, ImageFilter\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- ÏÑ§Ï†ï ---\n",
        "scale = 2\n",
        "patch_size = 33      # ÏûÖÎ†• (LR) Ìå®Ïπò ÌÅ¨Í∏∞\n",
        "label_size = 21      # Ï∂úÎ†• (SRCNN) ÌÅ¨Í∏∞ ‚Üí HR crop ÌÅ¨Í∏∞\n",
        "stride = 14\n",
        "blur_radius = 0.0   # Í∞ÄÏö∞ÏãúÏïà Î∏îÎü¨ Î∞òÏßÄÎ¶Ñ\n",
        "\n",
        "# --- Í≤ΩÎ°ú ÏÑ§Ï†ï ---\n",
        "# ÏïÑÎûò Í≤ΩÎ°úÎäî Ïã§Ï†ú ÌôòÍ≤ΩÏóê ÎßûÍ≤å ÏàòÏ†ïÌï¥Ï£ºÏÑ∏Ïöî.\n",
        "t91_dir = '/content/drive/MyDrive/SRCNN/T91_img'\n",
        "set5_dir = '/content/drive/MyDrive/SRCNN/Set5_img'\n",
        "output_dir = '/content/drive/MyDrive/SRCNN'\n",
        "\n",
        "# Ï∂úÎ†• ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "blur_str = str(int(blur_radius * 10))\n",
        "t91_output = os.path.join(output_dir, f'91-image_x{scale}_blur{blur_str}.h5')\n",
        "set5_output = os.path.join(output_dir, f'Set5_x{scale}_blur{blur_str}.h5')\n",
        "\n",
        "def convert_rgb_to_y(img):\n",
        "    img = img.astype(np.float32)\n",
        "    return 16.0 + (65.481 * img[..., 0] + 128.553 * img[..., 1] + 24.966 * img[..., 2]) / 255.0\n",
        "\n",
        "\n",
        "def preprocess_t91():\n",
        "    lr_patches = []\n",
        "    hr_patches = []\n",
        "\n",
        "    image_paths = sorted(glob.glob(os.path.join(t91_dir, '*')))\n",
        "    if not image_paths:\n",
        "        print(f\"Í≤ΩÍ≥†: '{t91_dir}' ÎîîÎ†âÌÜ†Î¶¨ÏóêÏÑú Ïù¥ÎØ∏ÏßÄÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.\")\n",
        "        return\n",
        "\n",
        "    for img_path in tqdm(image_paths):\n",
        "        hr = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # 1. Mod crop: Ïù¥ÎØ∏ÏßÄÎ•º scale Î∞∞ÏàòÎ°ú ÎÇòÎàÑÏñ¥Îñ®Ïñ¥ÏßÄÍ≤å ÌÅ¨Í∏∞ Ï°∞Ï†à\n",
        "        hr_width = (hr.width // scale) * scale\n",
        "        hr_height = (hr.height // scale) * scale\n",
        "        hr = hr.resize((hr_width, hr_height), resample=Image.BICUBIC)\n",
        "\n",
        "        # 2. LR ÏÉùÏÑ± (ÎÖºÎ¨∏ Î∞©Ïãù Ï†ÅÏö©)\n",
        "        #    a. Í∞ÄÏö∞ÏãúÏïà Î∏îÎü¨ Ï†ÅÏö©\n",
        "        hr_blurred = hr.filter(ImageFilter.GaussianBlur(radius=blur_radius))\n",
        "        #    b. Îã§Ïö¥ÏÉòÌîåÎßÅ (Ï†ÄÌï¥ÏÉÅÎèÑ ÏÉùÏÑ±)\n",
        "        lr = hr_blurred.resize((hr_width // scale, hr_height // scale), resample=Image.BICUBIC)\n",
        "        #    c. ÏóÖÏÉòÌîåÎßÅ (ÎÑ§Ìä∏ÏõåÌÅ¨ ÏûÖÎ†• ÌÅ¨Í∏∞Î°ú Î≥µÏõê)\n",
        "        lr = lr.resize((hr_width, hr_height), resample=Image.BICUBIC)\n",
        "\n",
        "        # 3. Y Ï±ÑÎÑê Ï∂îÏ∂ú\n",
        "        hr = convert_rgb_to_y(np.array(hr))\n",
        "        lr = convert_rgb_to_y(np.array(lr))\n",
        "\n",
        "        # 4. Ìå®Ïπò Ï∂îÏ∂ú (padding ÏóÜÎäî Íµ¨Ï°∞Ïóê ÎßûÏ∂§)\n",
        "        offset = (patch_size - label_size) // 2\n",
        "        for i in range(0, lr.shape[0] - patch_size + 1, stride):\n",
        "            for j in range(0, lr.shape[1] - patch_size + 1, stride):\n",
        "                lr_patch = lr[i:i+patch_size, j:j+patch_size]\n",
        "                hr_patch = hr[i+offset:i+offset+label_size, j+offset:j+offset+label_size]\n",
        "\n",
        "                lr_patches.append(lr_patch)\n",
        "                hr_patches.append(hr_patch)\n",
        "\n",
        "    # 5. HDF5 ÌååÏùºÎ°ú Ï†ÄÏû•\n",
        "    with h5py.File(t91_output, 'w') as f:\n",
        "        f.create_dataset('lr', data=np.array(lr_patches))\n",
        "        f.create_dataset('hr', data=np.array(hr_patches))\n",
        "    print(f\"‚úÖ Ï†ÄÏû• ÏôÑÎ£å: {t91_output}\")\n",
        "\n",
        "\n",
        "def preprocess_set5():\n",
        "    \"\"\"ÌèâÍ∞ÄÏö© Set5 Ïù¥ÎØ∏ÏßÄÎ•º Ï†ÑÏ≤òÎ¶¨ÌïòÏó¨ h5 ÌååÏùºÎ°ú Ï†ÄÏû•Ìï©ÎãàÎã§.\"\"\"\n",
        "    print(f\"üì¶ Set5 Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ Ï§ë (scale: x{scale}, blur_radius: {blur_radius})...\")\n",
        "\n",
        "    image_paths = sorted(glob.glob(os.path.join(set5_dir, '*')))\n",
        "    if not image_paths:\n",
        "        print(f\"Í≤ΩÍ≥†: '{set5_dir}' ÎîîÎ†âÌÜ†Î¶¨ÏóêÏÑú Ïù¥ÎØ∏ÏßÄÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.\")\n",
        "        return\n",
        "\n",
        "    with h5py.File(set5_output, 'w') as f:\n",
        "        f.create_group('lr')\n",
        "        f.create_group('hr')\n",
        "\n",
        "        for i, img_path in enumerate(image_paths):\n",
        "            hr = Image.open(img_path).convert('RGB')\n",
        "\n",
        "            # 1. Mod crop\n",
        "            hr_width = (hr.width // scale) * scale\n",
        "            hr_height = (hr.height // scale) * scale\n",
        "            hr = hr.resize((hr_width, hr_height), resample=Image.BICUBIC)\n",
        "\n",
        "            # 2. LR ÏÉùÏÑ± (ÎÖºÎ¨∏ Î∞©Ïãù Ï†ÅÏö©)\n",
        "            hr_blurred = hr.filter(ImageFilter.GaussianBlur(radius=blur_radius))\n",
        "            lr = hr_blurred.resize((hr_width // scale, hr_height // scale), resample=Image.BICUBIC)\n",
        "            lr = lr.resize((hr_width, hr_height), resample=Image.BICUBIC)\n",
        "\n",
        "            # 3. Y Ï±ÑÎÑê Ï∂îÏ∂ú\n",
        "            hr_y = convert_rgb_to_y(np.array(hr))\n",
        "            lr_y = convert_rgb_to_y(np.array(lr))\n",
        "\n",
        "            # 4. HDF5 ÌååÏùºÎ°ú Ï†ÄÏû•\n",
        "            f['hr'].create_dataset(str(i), data=hr_y)\n",
        "            f['lr'].create_dataset(str(i), data=lr_y)\n",
        "\n",
        "    print(f\"‚úÖ Ï†ÄÏû• ÏôÑÎ£å: {set5_output}\")\n",
        "\n",
        "\n",
        "# --- Ïã§Ìñâ ---\n",
        "# T91 Î∞è Set5 Ïù¥ÎØ∏ÏßÄ ÎîîÎ†âÌÜ†Î¶¨Í∞Ä Ï§ÄÎπÑÎêòÏóàÎäîÏßÄ ÌôïÏù∏ ÌõÑ Ïã§ÌñâÌïòÏÑ∏Ïöî.\n",
        "# Ïòà: ./T91_img/image1.bmp, ./Set5_img/baby.png\n",
        "if __name__ == '__main__':\n",
        "    preprocess_t91()\n",
        "    preprocess_set5()\n",
        "    print(\"Ïã§ÌñâÌïòÎ†§Î©¥ main Î∏îÎ°ùÏùò Ï£ºÏÑùÏùÑ Ìï¥Ï†úÌïòÍ≥†, Ïù¥ÎØ∏ÏßÄ Í≤ΩÎ°úÎ•º ÌôïÏù∏ÌïòÏÑ∏Ïöî.\")\n",
        "    print(\"ÏòàÏãú: T91_img, Set5_img ÎîîÎ†âÌÜ†Î¶¨Î•º ÌòÑÏû¨ Ìè¥ÎçîÏóê ÏÉùÏÑ±ÌïòÍ≥† Ïù¥ÎØ∏ÏßÄÎ•º ÎÑ£Ïñ¥Ï£ºÏÑ∏Ïöî.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "RCeuzo0VQlbx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}